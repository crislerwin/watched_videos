---
tags:
  - type/youtube
aliases: 
title: Ilya Sutskever (OpenAI Chief Scientist) - Building AGI, Alignment, Spies, Microsoft, & Enlightenment
channel_name: Dwarkesh Patel
subscribers: 47500
length: 47:41
publish_date: 2023-03-27
chapters:
  - 00:00 Time to AGI
  - 05:57 What’s after generative models?
  - 10:57 Data, models, and research
  - 15:27 Alignment
  - 20:53 Post AGI Future
  - 26:56 New ideas are overrated
  - 36:22 Is progress inevitable?
  - 41:27 Future Breakthroughs
hashtags: 
thumbnail: "![[1711477562504.jpg]]"
description: ""
note_created: 2024-03-26, 15:26
youtube_url: https://youtu.be/Yf1o0TQzry8
template-type: YouTube
template-version: "1.0"
created: 2024-03-26T15:26
updated: 2024-03-26T15:26
---

![[1711477562504.jpg]]
# SUMMARY

Ilya Sutskever, the Co-founder and Chief Scientist of OpenAI, is interviewed about his work in AI research, the progress and challenges of large language models, the path to AGI, and the importance of alignment and safety.

# IDEAS:

- Perseverance and hard work are key to making breakthroughs in research
- Tracking misuse of large language models like GPT is technically possible but challenging
- The economic value of AI will continue to grow exponentially until AGI is achieved
- Reliability and controllability are crucial emergent properties to strive for in AI systems
- Alignment research is an area where academic researchers can make meaningful contributions
- Inspiration from human intelligence should be applied carefully to avoid focusing on non-essential qualities
- Continued progress in AI will require a combination of new ideas, understanding existing models, and improving reliability and controllability

# INSIGHTS:

- Predicting the next token well requires understanding the underlying reality that led to the creation of that token, not just statistics
- Dedicated training and improvements to base models will help language models overcome limitations in multi-step reasoning
- Alignment of highly capable AI systems that could misrepresent their intentions will be a significant challenge
- Breakthroughs in AI often feel obvious in hindsight, but require overcoming non-obvious conceptual barriers

# QUOTES:

- "If your base neural net is smart enough, you just ask it — What would a person with great insight, wisdom, and capability do?"
- "Fundamentally I also don't feel like they're that bad at multi-step reasoning. I actually think that they are bad at mental multistep reasoning when they are not allowed to think out loud."
- "Reliability means you can trust the model's output, controllability means you can control it. And we'll see but it will be very cool if those emergent properties did exist."
- "I really feel like in some sense, the only thing that matters about hardware is cost per flop and overall systems cost."

# HABITS:

- Spending significant time understanding model behavior and results, not just coming up with new ideas
- Committing to building large-scale robotic systems to collect more data for progress
- Maintaining a long-term, patient perspective on AI progress and not getting overly optimistic

# FACTS:

- GPUs and TPUs are fundamentally similar in architecture, with the main difference being cost per flop
- Inference costs will likely be addressed through price discrimination and specialization, not a race to the bottom
- Alignment research is an area where academic researchers can make meaningful contributions

# REFERENCES:

- Reinforcement Learning from Human Feedback (RLHF)
- Forward forward algorithm
- ImageNet dataset

# RECOMMENDATIONS:

- Pursue research on reliability and controllability of large AI systems
- Explore ways to mathematically define and verify AI alignment
- Investigate emergent properties that could arise in massively scaled language models
- Consider how to best leverage human-AI collaboration for AI progress and safety

<iframe title="Ilya Sutskever (OpenAI Chief Scientist) - Building AGI, Alignment, Spies, Microsoft, & Enlightenment" src="https://www.youtube.com/embed/Yf1o0TQzry8?feature=oembed" height="113" width="200" style="aspect-ratio: 1.76991 / 1; width: 100%; height: 100%;" allowfullscreen="" allow="fullscreen"></iframe>

